网络包的收发过程图

![image](https://github.com/dwjlw1314/DWJ-PROJECT/raw/master/PictureSource/1.22.1.png)

```
一般应用程序发起一个网络请求，这个网络请求的数据会写到内核的套接字缓冲区当中，然后内核会对这个套接字缓冲区的数据去加上tcp头或udp头，然后又经由ip层，再加上一个ip头，
中间会经过防火墙的一系列规则对这个网络包进行过滤，看是丢弃还是继续往网卡上面去发送，最终到达链路层之后，这个网络包会经由链路层去发到网卡上的环形缓冲区上，
最后由网卡发送到整个网络当中，其中每一环都是有可能会发生丢包的
```

首先从系统层面看网络有几个重要的指标，MBS 代表网卡每秒发送多少或者是接收多少个M字节，Mbps是每秒多少M比特位。通常说的带宽的单位就是Mbps，一般100M带宽的话换算成MBS等于Mbps除以8

平时选择服务器节点的时候，除了带宽，还有pps就是每秒发送、接收包的数量，它也是有限制的

首先可以用sar观察机器节点两个指标的状态 #apt install sysstat
>[root@dwj ~]# sar -n DEV 1 5

查看整个系统层面的网络情况后，再用iftop从进程的角度去看这个问题 #apt install iftop

列出这个系统里面每一条链接的一个Mbps，能够找出哪个ip消耗流量最多。更多的时候其实不是系统网络达到瓶颈，而是进程处理网络包的能力跟不上
>[root@dwj ~]# iftop -P

列出每个进程的收发流量的数据，找出哪个进程是最消耗流量的，能够更方便的去定位哪个进程出的问题 #apt-get install nethogs
>[root@dwj ~]# nethogs ens33

使用go trace工具能够分析出网络调度带来的延迟问题，从侧面去反馈出程序在某一块代码上面可能是在进行频繁的网络调度，有可能是进行频繁调度之后，比较消耗带宽，从而可能间接的反映出延迟会略有提高

如何查找一个丢包问题，从上到下依次分析，先看应用层，通过listen这个方法去监听套接字实现三次握手的时候，会有两个队列

首先服务器接收到客户端的syn包的时候，会创建一个半连接队列，将那些还没有完成三次握手但是却发送了一个syn包的连接放到里面，同时回复客户端一个syn+ack

客户端收到了这个ack和syn包之后，回复给服务端一个ack，这个时候内核就会将这个连接放到全连接队列

当服务器调用accept方法的时候，会将这条连接从全连接队列里取出来，所以这个时候涉及了两个队列，如果这两个队列满了的话，就会可能会产生丢包的行为

![image](https://github.com/dwjlw1314/DWJ-PROJECT/raw/master/PictureSource/1.22.2.jpg)

![image](https://github.com/dwjlw1314/DWJ-PROJECT/raw/master/PictureSource/1.22.3.jpg)

如何去确定是由于半连接队列溢出导致的丢包?

通过dmesg去日志里面去搜寻tcp drop，能够发现丢包的情况，dmesg是一个内核的日志记录，能够从里面去找出一些内核的行为
>[root@dwj ~]# dmesg|grep "TCP: drop open erquest form"

全连接队列该怎么看，通过ss命令能够去看到你的服务在listen的时候，全连接队列的大小
>[root@dwj ~]# ss -lnt  ## -l 显示正在监听 -n 不解析服务名称 -t 只显示 tcp socket

```
Send-Q 是代表当前全连接队列长度，也就是当前已完成三次握手并等待服务端 accept() 的 TCP 连接
Recv-Q 是指当前全连接队列的大小，上面的输出结果说明监听 9000 端口的 TCP 服务，最大全连接长度为 128
Recv-Q 一般都是为0，如果存在一种大于0的情况并且会持续一个较长时间的话，就说明服务处理连接的能力比较慢了，会导致全连接队列过满或者丢弃，这个时候应该会加快你的服务处理连接的能力

ss命令对于状态为ESTABLISHED的连接，它看的不是这个监听服务，而是去看一条已经建立好的连接相关指标
Recv-Q 是代表收到但未被应用程序读取的一个字节数
Send-Q 已发送但未收到确认的字节数
通过这两个指标，能够去看到是应用程序对一个数据的处理能力慢，还是说是客户端对接收的数据处理的比较慢的情况，一般这两个值也都是为0，如果有其中一个不为0，你可能要去排查一下是客户端的问题还是服务器的问题

当全连接队列满了之后，内核默认会将包丢弃，可指定内核参数修改
tcp_abort_on_overflow=1 表示会直接发一个reset的包给客户端，直接将这个连接断开，废掉这个握手和连接过程
```

经过应用层之后，网络包会到达到传输层，传输层会有防火墙的存在，如果防火墙开启的话，需要查看防火墙连接跟表

nf_conntrack 这个是linux为每个经过内核网络栈的数据包生成一个连接的记录项

当服务器处理过多时，这个连接记录项所在的连接跟踪表就会被打满，然后服务器就会丢弃新建连接的数据包，这时需要查看防火墙的连接跟踪表的太小了


那如何去看连接跟踪表的大小呢

查看nf_conntrack表最大连接数
>[root@dwj ~]# cat /proc/sys/net/netfilter/nf_conntrack_max

查看nf_conntrack表当前连接数
>[root@dwj ~]# cat /proc/sys/net/netfilter/nf_conntrack_count

网络包经过传输层之后，再来看网络层和物理层，提到网络层和物理层，就要看网卡了，通过netstat命令，能够去看整个机器上面网卡的丢包和收包的情况
>[root@dwj ~]# cat /proc/sys/net/netfilter/nf_conntrack_max   # netstat可以统计网路丢包以及环形缓冲区溢出

RX-DRP 这个指标数据，如果它大于0，说明这个网卡是有丢包情况，这里记录的是从开机到目前为止的数据情况，所以在分析的时候，隔一定的时间去看这个指标是否有上涨

RX-OVR 指标说明这个网卡的环形缓冲区满了之后产生的丢弃行为

netstat还能够统计网络协议层的丢包情况

![image](https://github.com/dwjlw1314/DWJ-PROJECT/raw/master/PictureSource/1.22.4.jpg)

MTU

应用层的网络包通过网络层的时候会根据数据包的大小去进行分包发送

当tcp数据包的大小发送网络层之后，网络层发现这个包大于它的mtu值，这个数据包会进行一个分包的操作

在进行网卡设置的时候，会设置为你的传输层包，如果大于了mtu这个值，那就可以直接将这个网络包丢弃，这也是在现实生活中经常会碰到的一个丢包问题

所以在检查链路的时候，通常链路长了可能不太好排查，链路短一点，可能会很容易看到整条链路当中mtu的情况，看一下是不是每条链路上对应的每个网卡的mtu指标是不一样的

如果不一样的话，可能会造成丢包，因为一个包的转发跟网络上面设置的mtu值大小有关系，比如设置为大于mtu之后会把这个包给丢弃掉

如果发送的mtu包的大小超过网卡规定的大小，并且网卡不允许分片，那么则会产生丢包
